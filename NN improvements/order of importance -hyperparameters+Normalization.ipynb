{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's arange the hyperparameter in order of importance, by order of importance, I mean which one is worth tuning first\n",
    "\n",
    "1. learning Rate\n",
    "2. momentum (usually .9)/ # hidden units / mini batch size (should be $2^n$)\n",
    "3. \\# of layers / learning rate decay\n",
    "4. (rarely) Adam parameters : $\\beta_1 $, $\\beta_2 $, and $\\epsilon $ (usually: .9 / .999 / and $10^{-8}$ , respectively)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameter tuning strategies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## when trying hyperparameters out, don't use a grid but select a batch of values from the hyperparameters you want to tune at random, it may save you some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## focusing on a smaller subregion, when noticing that some hyperparamters within that region work best\n",
    "<img src = './data/coarse_to_fine.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example of choosing hyperparameter values:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  alpha between .0001 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.17570041e-02, 1.57450040e-04, 1.18038650e-03, 4.03056624e-03,\n",
       "        2.37643225e-01, 2.37696025e-01, 5.85686955e-01, 3.43001024e-04,\n",
       "        3.94039676e-03, 1.47132864e-03]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "# we can't just take random values between .0001 and 1 because few values will end up in the lower end, and most of them \n",
    "# will end up between .1 and 1\n",
    "# so we do\n",
    "r = -4  * np.random.rand(1,10)\n",
    "alpha = 10**r\n",
    "np.printoptions(supress = True,precision = 2)\n",
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## momentum between .9 and .999\n",
    "\n",
    "#### this one is tricky because if you remember well, .9 is like taking the moving average over $ \\frac{1}{1-.9}=10 $ values, whereas, .999 is like taking the moving average over $ \\frac{1}{1-.999}=1000 $ values. so it doesn't make sense to scale on the uniform scale between .9 and .999,  so a better way of thinking about it is that we want to explore the range of values of 1- $\\beta$, which is now going to range between .1 and .001, and sample between those values, like we did for alpha above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99438848, 0.92030545, 0.97089364, 0.98424868, 0.99794866,\n",
       "        0.99794889, 0.99869333, 0.94600516, 0.98406948, 0.97392975]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "r = np.random.uniform(low=-3.0, high=-1.0, size=(1,10)) # because r is ranging between -3 and -1\n",
    "# or -2 * np.random.rand(1,10) - 1\n",
    "beta = 1 - 10**r\n",
    "\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization :\n",
    "\n",
    "#### before we were usually normalizing the input parameters X, and its features, but what if we also normalized parameters within the NN, it is called batch normalization, and it is the process of normalizing $Z^{[i]}$ or $A^{[i]}$ .\n",
    "#### the learnable parameters make sure that the mean and variance are controlled, and thus choose a good mean, and std\n",
    "\n",
    "<img src = './data/Batch_Norm.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './data/BNasReg.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization at Test time\n",
    "#### the parameters in the blue box (mean, and std) at computed at batch time during the training, but for the testing , you are predicting each example independently from the others, so we can't do that. So we have to come up with another way to find the mean and std (because taking the mean and std of a single example doesn't make sense).\n",
    "<img src = './data/BNATT.png'>\n",
    "\n",
    "### the answer is estimating $\\mu$ and $\\sigma^2$ using exponentially weighted average across mini batches for each layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
