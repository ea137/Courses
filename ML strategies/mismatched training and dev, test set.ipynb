{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's say youn want to build an app, and one of its functionalities is allowing users to upload pictures. and you want to be able to classify as cat/non-cat. And you already have a trained algorithm. What's the next test ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data from webpages : 200,000 images\n",
    "# Data uploaded : 10,000 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. you can use the uploaded images as a new test set, and if it acheives high accuracy (or whatever other metric you chose), then its fine.\n",
    "2. else if you are not content with that assesment, you have 2 options :\n",
    "    * Option 1: add the uploaded images to your dataset, reshuffle the whole thing then split it to train, dev, and test set. but in this case, it is not the best option because the data from webpages overpowers the uploaded one. So the dev and test set will be mostly comprised of the webpages data. Which means that with your dev and test set, you re assessing how well you are predicting webpages data instead of uploaded data, in other words, you are aiming at the wrong target.\n",
    "    * option 2: add a part of the uploaded images to the training set, and split the remainings between the dev and test set. this option is preferred in this case because the dev and test set are assessing your predictions on uploaded data, which is the correct target.\n",
    "    \n",
    "###### But with option 2, the training data is of a different distribution from the dev and test set. But it turns out that over the long term, it is giving better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's say you chose option 2, then you get new results :\n",
    "\n",
    "* human errors $\\approx$ 0%\n",
    "* training error 1 %\n",
    "* Dev error 10 %\n",
    "\n",
    "###### you might want to say that you have a variance problem, but you can't tell for sure because the distributions of the train and dev test are different, so how can we remedy to that ?\n",
    "you can take a small part of the training data and call it train-dev test which will have the same distribution as the training set. PS : train-dev is not used for training.\n",
    "\n",
    "###### after applying this, those are the new results : \n",
    "* human errors $\\approx$ 0%\n",
    "* training error 1 %\n",
    "* train-dev error 9 %\n",
    "* Dev error 10 %\n",
    "\n",
    "$\\Rightarrow$ here it becomes more evident that it is truly a variance problem.\n",
    "\n",
    "###### let's consider a new scenario : \n",
    "* human errors $\\approx$ 0%\n",
    "* training error 1 %\n",
    "* train-dev error 1.5 %\n",
    "* Dev error 10 %\n",
    "\n",
    "$\\Rightarrow$ here, it is a data mismatch problem because whatever what the algorithm is learning, it can't generalize it to the dev test which has a different distribution\n",
    "\n",
    "###### let's consider another scenario : \n",
    "* human errors $\\approx$ 0%\n",
    "* training error 10 %\n",
    "* train-dev error 11 %\n",
    "* Dev error 12 %\n",
    "\n",
    "$\\Rightarrow$ here it is a bias problem, the model is underfitting the data.\n",
    "\n",
    "###### let's consider another scenario : \n",
    "* human errors $\\approx$ 0%\n",
    "* training error 10 %\n",
    "* train-dev error 11 %\n",
    "* Dev error 20 %\n",
    "\n",
    "$\\Rightarrow$ here it is both a bias and data mismatch problem.\n",
    "\n",
    "###### let's consider one last scenario : \n",
    "* human errors $\\approx$ 0%\n",
    "* training error 1 %\n",
    "* train-dev error 1.5 %\n",
    "* Dev error 10 %\n",
    "\n",
    "$\\Rightarrow$ here you may think that there is a mismatch problem, but let's say that you assign a task to humans to label the dev and test set, you find that their error is equal to 9 % which means that because the dev set is comming from a different distribution, it is harder to to classify and thus less resilient to errors, so it is not really a data mismatch problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### let's go over one last case, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './data/B_V_M.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing Data Mismatch:\n",
    "\n",
    "* Carry out manual error analysis to try to understand the difference between training, dev/test sets\n",
    "* Make training data more similar, or collect more data similar to dev/test sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
