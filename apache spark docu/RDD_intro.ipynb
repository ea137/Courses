{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilient Distributed Dataset (RDD):\n",
    "\n",
    "1. Distributed, immutable collection of data . <br>\n",
    "2. created from HDFS, ObjectStore, simple files, NoSQL, SQL... . <br>\n",
    "3. In memory, but spillable to disk: <span style = 'color:yellow'> Once an RDD is created, it resides distributed in the main memory of the different worker nodes, but when the aggregated main memory of all the worker nodes is not sufficient then data get split to disk .  <br>\n",
    "4. Lazy : <span style = 'color:yellow'> Data stored in RDDs is only read from the underlying storage systems when needed. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining HDFS and Worker nodes:\n",
    "\n",
    "<font style  = 'color:white' size='0'>Apache Spark is written in scala, a Java virtual machine (JVM) compatible programming language.Let's consider your *data analysis application* is running on a single JVM, and therefore is limited to the resources a single nodes provides.If we want to use large compute clusters but still don't want to take care of parallelizing our programs, Apache Spark Kicks in. <br><font style  = 'color:lightblue' size='4'> Let's turn our single node JVM application into a *driver program* which *not* involved in any computations anymore but only managing remote compute nodes, which are called worker Nodes.<br> Those remote worker are responsible for executing your parallel version of your analytics workflow.[1]<br>\n",
    "Since the driver only talks to multiple remove JVMs, it doesn't matter on which worker node it resides.[2] <br> But what about the data, does it matter where the data resides ? there are two options for attaching to an Apache spark cluster:\n",
    "1. Using an off node storage approach (simplest one), where the third system is attached to the cluster using a *fast* network connection.[3]\n",
    "2. Attach hard drives to the worker nodes (JBOD , just of discs). In order to retrieve the combined storage capacity of all disks as one large virtual file system we have to add a software component to the cluster, HDFS (Hadoop distributed file system). Rest APIs are used to interact with this file system. <span style='color:yellow'> Let's consider a file too big to fit on a single disk, then we will divid it into equal size chunks and distribute them over physical disks, then HDFS creates a virtual view on top of those chunks so that it can be treated as a single large file spanning the whole cluster.[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]\n",
    "<img src = './data/worker_nodes.png' title = '[1]' height = '200' width = '200'>\n",
    "[2]\n",
    "<img src = './data/worker_nodes2.png' title = '[2]' height = '200' width = '200'>\n",
    "[3]\n",
    "<img src = './data/worker_nodes3.png' title = '[3]' height = '200' width = '200'>\n",
    "[4]\n",
    "<img src = './data/worker_nodes4.png' title = '[4]' height = '200' width = '200'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))  \n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "#creating rdd\n",
    "rdd = sc.parallelize(range(100))\n",
    "# number of elements in RDD\n",
    "print(rdd.count()) # triggers the execution of a spark trip \n",
    "               # which gets divided into individual spark\n",
    "               # tasks\n",
    "# those spark tasks are executed in parallel on the \n",
    "# cluster using the spark executor JVMs\n",
    "print(rdd.take(10)) # first 10 elements in this rdd\n",
    "print(rdd.collect()) # all elements in rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "4950\n"
     ]
    }
   ],
   "source": [
    "# more examples\n",
    "print(rdd.map(lambda x : x+10).take(10))\n",
    "print(rdd.reduce(lambda a,b : a+b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
