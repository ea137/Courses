{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext.getOrCreate(SparkConf())\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def flatten(x,temp = []):\n",
    "    '''\n",
    "    flatten tuple \n",
    "    '''\n",
    "    if type(x) == int or type(x) == float:\n",
    "        return x\n",
    "    for val in x:\n",
    "        value = flatten(val,temp)\n",
    "        if type(value)== int or type(value) == float:\n",
    "            temp.append(value)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averge : 49.5\n",
      "Median : 49.5\n",
      "Standard Deviation : 28.86607004772212\n",
      "Skewness : 1.4210854715202004e-16\n",
      "Kurtosis : 1.7997599759975997\n",
      "Correlation : 0.9676443927131408\n",
      "Correlation Matrix :\n",
      " [[1.                nan 1.         0.96764439 0.91551566]\n",
      " [       nan 1.                nan        nan        nan]\n",
      " [1.                nan 1.         0.96764439 0.91551566]\n",
      " [0.96764439        nan 0.96764439 1.         0.98595001]\n",
      " [0.91551566        nan 0.91551566 0.98595001 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# average\n",
    "avg = rdd.sum() / rdd.count()\n",
    "print('Averge :' ,avg)\n",
    "\n",
    "# median\n",
    "temp = rdd.sortBy(lambda x :x , ascending = False).zipWithIndex().map(lambda x: x[::-1] )       \n",
    "n = temp.count()\n",
    "if n% 2 ==1:\n",
    "    median = temp.lookup((n-1)/2)[0]\n",
    "if n% 2 ==0:\n",
    "    median = (temp.lookup(n/2)[0] + temp.lookup(n/2 -1)[0])/2\n",
    "print('Median :',median)\n",
    "\n",
    "# Standard Deviation\n",
    "N = rdd.count()\n",
    "std = np.sqrt( 1/N *  rdd.map(lambda x : (x-avg)**2).sum()  )\n",
    "print('Standard Deviation :',std) # we can be using median+standard deviation to calculate the threshold\n",
    "\n",
    "# Skewness :\n",
    "# Negative values for the skewness indicate data that are skewed left and positive values for the skewness\n",
    "# indicate data that are skewed right. By skewed left, we mean that the left tail is long relative to the\n",
    "# right tail. Similarly, skewed right means that the right tail is long relative to the left tail.\n",
    "skew = 1/N * rdd.map(lambda x : ((x - avg)**3)/std**3  ).sum()\n",
    "print('Skewness :', skew)\n",
    "\n",
    "# Kurtosis : \n",
    "# Reports on the shape of the data when ploted using a histogram, and the outlier content within the data\n",
    "# in other words, the length of the tails of the histogram.\n",
    "# the higher the Kurtosis the more outliers are present and the longer the tails of the distribution in the histogram are\n",
    "kurtosis = 1/N * rdd.map(lambda x : (x-avg)**4/std**4 ).sum()\n",
    "print('Kurtosis :', kurtosis)\n",
    "\n",
    "# Correlation\n",
    "rdd2 = rdd.map(lambda x : x**2 + 30) # creating new values\n",
    "avg2 = rdd2.sum() / N\n",
    "std2 = np.sqrt( 1/N *  rdd2.map(lambda x : (x-avg2)**2).sum()  )\n",
    "covariance = 1/N * rdd.zip(rdd2).map(lambda x : (x[0]-avg)* (x[1]-avg2) ).sum()\n",
    "correlation = covariance / (std*std2 )\n",
    "print('Correlation :', correlation)\n",
    "\n",
    "# Correlation Matrix\n",
    "# creating rdds \n",
    "new_ = rdd\n",
    "for i in range(4):\n",
    "    rnd = random.randint(0,100)\n",
    "    globals()['rdd'+str(i)] = rdd.map(lambda x : x**i +rnd )\n",
    "    temp = globals()['rdd'+str(i)]\n",
    "    new_ = new_.zip(temp)\n",
    "    #print('rdd'+str(i),True)\n",
    "from pyspark.mllib.stat import Statistics\n",
    "data = new_.map(lambda x : flatten(x,temp = []) )\n",
    "print('Correlation Matrix :\\n',Statistics.corr(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6500000000000004"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "N = rdd.count()\n",
    "avg = rdd.sum() / N\n",
    "std = np.sqrt( 1/N *  rdd.map(lambda x : (x-avg)**2).sum()  )\n",
    "rdd2 = sc.parallelize([7,6,5,4,5,6,7,8,9,10])\n",
    "avg2 = rdd2.sum() / N\n",
    "std2 = np.sqrt( 1/N *  rdd2.map(lambda x : (x-avg2)**2).sum()  )\n",
    "1/N * rdd.zip(rdd2).map(lambda x : (x[0]-avg)* (x[1]-avg2) ).sum() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
